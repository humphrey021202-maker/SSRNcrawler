# src/<ä½ çš„åŒ…>/runner.py
from __future__ import annotations
from typing import List, Dict, Set
import asyncio, os, random
from collections import deque

from .config import (
    DATA_DIR, JOURNAL_IDS, JOURNAL_PAGE_RANGE, JOURNAL_URL_TEMPLATE,
    GLOBAL_DETAIL_CONCURRENCY, ENABLE_GLOBAL_DEDUP,
    BACKOFF_BASE, BACKOFF_MAX, CHECKPOINT_FILE,
)
from .checkpoint import save_checkpoint, load_checkpoint
from .utils import (
    polite_sleep, humanize_page, load_links_on_page, looks_like_challenge,
    extract_abstract_id_from_url, make_filename
)
from .scraping import fetch_article_text

# å…¨å±€å¹¶å‘ä»é™å®šä¸º 1ï¼Œæ›´ç¨³
detail_sem = asyncio.Semaphore(GLOBAL_DETAIL_CONCURRENCY)
seen_ids_lock = asyncio.Lock()
seen_ids: Set[str] = set()

LINK_SELECTORS = [
    'a[href*="papers.cfm?abstract_id="]',
    "div.title a",
    ".abstract-title a",
    "a.search-result-title",
]

def save_progress(journals: List[Dict], current_index: int, current_page: int,
                  current_link_idx: int, seen_ids: Set[str]) -> None:
    """
    journals: [{"name":..., "jid":..., "page":..., "link_idx":..., "save_dir":...}, ...]
    current_index: æ­£åœ¨å¤„ç†çš„æœŸåˆŠåœ¨ journals åˆ—è¡¨ä¸­çš„ä¸‹æ ‡
    current_page:  æ­£åœ¨å¤„ç†çš„é¡µç 
    current_link_idx: å½“å‰é¡µé‡Œå·²ç»å¤„ç†åˆ°çš„é“¾æ¥åºå·ï¼ˆä» 0 å¼€å§‹ï¼›å¦‚æœåˆšå¤„ç†å®Œç¬¬ k æ¡ï¼Œå°±ä¼  kï¼‰
    """
    snapshot = []

    # 1) å½“å‰æœŸåˆŠçš„æœ€æ–°æ¸¸æ ‡
    cur = journals[current_index]
    snapshot.append({
        "name": cur["name"],
        "jid": cur["jid"],
        "page": int(current_page),
        "end_page": 999999,          # é¡ºåºæ¨¡å¼ä¸‹ç”¨ä¸åˆ°ï¼Œä¿ç•™å­—æ®µå³å¯
        "link_idx": int(current_link_idx),
        "save_dir": cur["save_dir"],
        "article_idx": 0,            # é¡ºåºæ¨¡å¼ä¸å†ç”¨ç¼–å·å†™åï¼Œå¯å ä½
    })

    # 2) å…¶åçš„å‰©ä½™æœŸåˆŠï¼ˆä¿æŒåŸé¡ºåºï¼‰
    for j in range(current_index + 1, len(journals)):
        rest = journals[j]
        snapshot.append({
            "name": rest["name"],
            "jid": rest["jid"],
            "page": int(rest.get("page", 1)),
            "end_page": 999999,
            "link_idx": int(rest.get("link_idx", 0)),
            "save_dir": rest["save_dir"],
            "article_idx": 0,
        })

    # å†™ç›˜
    save_checkpoint(deque(snapshot), seen_ids)

async def scrape_all_journals_rotating(context) -> None:
    """
    é¡ºåºæŠ“å–æ¯æœ¬æœŸåˆŠï¼Œè‡ªåŠ¨æ¢æµ‹å°¾é¡µï¼š
    - åˆ—è¡¨é¡µ/è¯¦æƒ…é¡µå‡ºç°ä¸€æ¬¡éªŒè¯ => éšæœºå†·å´ BACKOFF_BASE~BACKOFF_MAX ç§’ï¼Œç„¶ååŸåœ°é‡è¯•/ç»§ç»­
    - åˆ—è¡¨é¡µè¿ç»­ä¸¤é¡µæ— é“¾æ¥ï¼ˆä¸”éæŒ‘æˆ˜é¡µï¼‰ => åˆ¤å®šåˆ°å°¾é¡µ
    - ç½‘ç»œ/è¶…æ—¶é”™è¯¯ï¼šä¸åŠ é¡µï¼ŒåŸåœ°å†·å´åé‡è¯•
    - æ–­ç‚¹ï¼šç²¾ç¡®åˆ°â€œç¬¬ N é¡µçš„ç¬¬ K æ¡â€ï¼›Ctrl+C æ—¶ä¹Ÿä¼šè½ç›˜
    - ä¿å­˜è·¯å¾„ï¼šdata/<æœŸåˆŠå>/ï¼›æ–‡ä»¶åï¼š{abstract_id}_page{page}_NO.{ordinal}.txt
    """
    restored = load_checkpoint(CHECKPOINT_FILE)
    journals: List[Dict] = []

    if restored and restored.get("cursors"):
        print(f"ğŸ” å‘ç°æ–­ç‚¹æ–‡ä»¶ {CHECKPOINT_FILE}ï¼Œå°†ä»ä¸Šæ¬¡ä½ç½®ç»§ç»­ã€‚")
        for cur in restored["cursors"]:
            journals.append({
                "name": cur["name"],
                "jid": cur["jid"],
                "page": max(1, int(cur["page"])),
                "link_idx": max(0, int(cur["link_idx"])),
                "save_dir": os.path.join(DATA_DIR, cur["name"]),
            })
    else:
        for name, jid in JOURNAL_IDS.items():
            sp, _ = JOURNAL_PAGE_RANGE.get(name, (1, 1))
            journals.append({
                "name": name,
                "jid": jid,
                "page": sp,
                "link_idx": 0,
                "save_dir": os.path.join(DATA_DIR, name),
            })

    list_page = await context.new_page()
    await humanize_page(list_page)

    try:
        for i, cur in enumerate(journals):
            name = cur["name"]; jid = cur["jid"]; page_num = int(cur["page"])
            os.makedirs(cur["save_dir"], exist_ok=True)

            print(f"\n===== å¼€å§‹æœŸåˆŠ {name} (jid={jid})ï¼Œä»ç¬¬ {page_num} é¡µèµ· =====")

            empty_pages_in_a_row = 0

            while True:
                list_url = JOURNAL_URL_TEMPLATE.format(jid=jid, page=page_num)
                print(f"\nğŸŒ [{name}] ç¬¬ {page_num} é¡µ: {list_url}")

                # åˆ—è¡¨é¡µï¼šç½‘ç»œ/è¶…æ—¶é”™è¯¯ -> ä¸åŠ é¡µï¼ŒåŸåœ°é€€é¿é‡è¯•
                try:
                    links = await load_links_on_page(list_page, list_url, LINK_SELECTORS)
                except Exception as e:
                    wait_s = random.uniform(BACKOFF_BASE, BACKOFF_MAX)
                    print(f"âš ï¸ åˆ—è¡¨åŠ è½½å¤±è´¥ï¼š{e}\n   â‡¢ å†·å´ {wait_s:.1f}s ååœ¨åŒä¸€é¡µé‡è¯• â€¦")
                    save_progress(journals, i, page_num, cur.get("link_idx", 0), seen_ids)
                    await asyncio.sleep(wait_s)
                    continue

                # æ— é“¾æ¥ï¼šæŒ‘æˆ˜ or çœŸç©ºé¡µ
                if not links:
                    try:
                        body = await list_page.inner_text("body")
                    except Exception:
                        body = ""

                    # éªŒè¯é¡µï¼šä¸åŠ é¡µï¼ŒåŸåœ°é€€é¿é‡è¯•
                    if looks_like_challenge(body):
                        wait_s = random.uniform(BACKOFF_BASE, BACKOFF_MAX)
                        print(f"ğŸ§± åˆ—è¡¨é¡µç–‘ä¼¼éªŒè¯ï¼Œå†·å´ {wait_s:.1f}s åé‡è¯•å½“å‰é¡µ â€¦")
                        save_progress(journals, i, page_num, cur.get("link_idx", 0), seen_ids)
                        await asyncio.sleep(wait_s)
                        continue

                    # çœŸç©ºé¡µï¼šè¿ç»­ä¸¤æ¬¡æ‰åˆ¤å°¾é¡µ
                    empty_pages_in_a_row += 1
                    if empty_pages_in_a_row >= 2:
                        print(f"âœ… [{name}] è¿ç»­ä¸¤é¡µæ— æœ‰æ•ˆé“¾æ¥ï¼ˆåˆ°ç¬¬ {page_num} é¡µï¼‰ï¼Œåˆ¤å®šåˆ°å°¾é¡µï¼Œç»“æŸè¯¥åˆŠã€‚")
                        # æ–­ç‚¹ï¼šå†™ä¸‹ä¸€ä¸ªæœŸåˆŠçš„èµ·ç‚¹ï¼ˆæˆ–ä¿ç•™å½“å‰å¿«ç…§ä¹Ÿè¡Œï¼‰
                        save_progress(journals, i, page_num, cur.get("link_idx", 0), seen_ids)
                        break
                    else:
                        print(f"ğŸ“­ [{name}] ç¬¬ {page_num} é¡µæ— æœ‰æ•ˆé“¾æ¥ï¼Œç¿»åˆ°ä¸‹ä¸€é¡µç¡®è®¤ã€‚")
                        # ç¿»é¡µå‰è½ç›˜ï¼ˆä¸‹ä¸€é¡µä» 0 å¼€å§‹ï¼‰
                        save_progress(journals, i, page_num, 0, seen_ids)
                        cur["link_idx"] = 0
                        page_num += 1
                        continue
                else:
                    empty_pages_in_a_row = 0  # æœ‰é“¾æ¥å³æ¸…é›¶

                print(f"ğŸ“‘ [{name}] ç¬¬ {page_num} é¡µå…± {len(links)} æ¡æ–‡ç« é“¾æ¥")

                # è‹¥ä»æ–­ç‚¹æ¢å¤ï¼šè·³è¿‡å·²å¤„ç†å®Œçš„æœ¬é¡µå‰è‹¥å¹²æ¡ï¼ˆlink_idx è¡¨ç¤ºâ€œå·²å¤„ç†åˆ°çš„åºå·â€ï¼‰
                start_ordinal = max(1, int(cur.get("link_idx", 0)) + 1)

                for ordinal, link in enumerate(links, start=1):
                    if ordinal < start_ordinal:
                        continue  # è·³è¿‡å·²å¤„ç†æ¡ç›®

                    abs_id = extract_abstract_id_from_url(link) or f"unk_{ordinal}"
                    file_stem = make_filename(abs_id, page_num, ordinal)

                    # å»é‡ï¼ˆå¯é€‰ï¼‰
                    if ENABLE_GLOBAL_DEDUP and not abs_id.startswith("unk_"):
                        async with seen_ids_lock:
                            if abs_id in seen_ids:
                                print(f"â†©ï¸ å·²æŠ“è¿‡ abstract_id={abs_id}ï¼Œè·³è¿‡")
                                # è¿›åº¦å‰ç§»ï¼šå·²å¤„ç†åˆ° ordinal
                                cur["link_idx"] = ordinal
                                save_progress(journals, i, page_num, ordinal, seen_ids)
                                continue

                    async with detail_sem:
                        saved, hit_challenge, _ = await fetch_article_text(
                            context=context,
                            url=link,
                            save_dir=cur["save_dir"],
                            file_stem=file_stem,
                        )

                    # å»é‡é›†åˆå†™å…¥
                    if ENABLE_GLOBAL_DEDUP and saved and not abs_id.startswith("unk_"):
                        async with seen_ids_lock:
                            seen_ids.add(abs_id)

                    # è¿›åº¦å‰ç§»ï¼šå·²å¤„ç†åˆ° ordinal
                    cur["link_idx"] = ordinal
                    save_progress(journals, i, page_num, ordinal, seen_ids)

                    # è¯¦æƒ…é¡µå‘½ä¸­æŒ‘æˆ˜ï¼šéšæœºé€€é¿ï¼ŒåŸåœ°ç»§ç»­æœ¬é¡µ
                    if hit_challenge:
                        wait_s = random.uniform(BACKOFF_BASE, BACKOFF_MAX)
                        print(f"ğŸ§± è¯¦æƒ…é¡µç–‘ä¼¼éªŒè¯ï¼Œå†·å´ {wait_s:.1f}s åç»§ç»­æœ¬é¡µ â€¦")
                        save_progress(journals, i, page_num, ordinal, seen_ids)
                        await asyncio.sleep(wait_s)

                # æœ¬é¡µå¤„ç†å®Œ -> ç¿»é¡µï¼ˆé¡µå†…ä½ç½®å½’é›¶ï¼‰
                cur["link_idx"] = 0
                save_progress(journals, i, page_num, 0, seen_ids)
                page_num += 1

            print(f"ğŸ¯ æœŸåˆŠ {name} å®Œæˆã€‚")

    except KeyboardInterrupt:
        # Ctrl+Cï¼šå°½åŠ›ä¿å­˜å½“å‰ä½ç½®
        try:
            # æ‰¾åˆ°ä¸€ä¸ªå°½å¯èƒ½å®‰å…¨çš„ä¸‹æ ‡å’Œä½ç½®ä¿å­˜
            # è¿™é‡Œå‡è®¾ i/page_num/cur["link_idx"] ä»åœ¨ä½œç”¨åŸŸå†…ï¼ˆè‹¥å¼‚å¸¸æ—©äºå®šä¹‰ï¼Œå¯å†åšä¿æŠ¤ï¼‰
            if 'i' in locals() and 'page_num' in locals():
                lk = 0
                try:
                    lk = journals[i].get("link_idx", 0)
                except Exception:
                    pass
                save_progress(journals, i, page_num, lk, seen_ids)
            print("ğŸ›‘ æ•è·åˆ° Ctrl+Cï¼Œå·²ä¿å­˜æ–­ç‚¹ã€‚")
        except Exception as e:
            print(f"âš ï¸ Ctrl+C æ—¶ä¿å­˜æ–­ç‚¹å¤±è´¥ï¼š{e}")
    finally:
        try:
            await list_page.close()
        except Exception:
            pass

    print("\nğŸ‰ æ‰€æœ‰æœŸåˆŠå¤„ç†å®Œæˆ")
