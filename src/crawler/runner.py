# src/<ä½ çš„åŒ…>/runner.py
from __future__ import annotations
from typing import List, Dict, Set
import asyncio, os, random
from collections import deque

from .config import (
    DATA_DIR, JOURNAL_IDS, JOURNAL_PAGE_RANGE, JOURNAL_URL_TEMPLATE,
    GLOBAL_DETAIL_CONCURRENCY, ENABLE_GLOBAL_DEDUP,
    BACKOFF_BASE, BACKOFF_MAX, CHECKPOINT_FILE, RETRY_PER_ARTICLE,   # â† å¼•å…¥é‡è¯•ä¸Šé™
)
from .checkpoint import save_checkpoint, load_checkpoint
from .utils import (
    polite_sleep, humanize_page, load_links_on_page, looks_like_challenge,
    extract_abstract_id_from_url, make_filename
)
from .scraping import fetch_article_text

# å…¨å±€å¹¶å‘ä»é™å®šä¸º 1ï¼Œæ›´ç¨³
detail_sem = asyncio.Semaphore(GLOBAL_DETAIL_CONCURRENCY)
seen_ids_lock = asyncio.Lock()
seen_ids: Set[str] = set()

LINK_SELECTORS = [
    'a[href*="papers.cfm?abstract_id="]',
    "div.title a",
    ".abstract-title a",
    "a.search-result-title",
]

def save_progress(journals: List[Dict], current_index: int, current_page: int,
                  current_link_idx: int, seen_ids: Set[str]) -> None:
    snapshot = []

    # 1) å½“å‰æœŸåˆŠçš„æœ€æ–°æ¸¸æ ‡
    cur = journals[current_index]
    snapshot.append({
        "name": cur["name"],
        "jid": cur["jid"],
        "page": int(current_page),
        "end_page": 999999,
        "link_idx": int(current_link_idx),
        "save_dir": cur["save_dir"],
        "article_idx": 0,
    })

    # 2) å…¶åçš„å‰©ä½™æœŸåˆŠï¼ˆä¿æŒåŸé¡ºåºï¼‰
    for j in range(current_index + 1, len(journals)):
        rest = journals[j]
        snapshot.append({
            "name": rest["name"],
            "jid": rest["jid"],
            "page": int(rest.get("page", 1)),
            "end_page": 999999,
            "link_idx": int(rest.get("link_idx", 0)),
            "save_dir": rest["save_dir"],
            "article_idx": 0,
        })

    save_checkpoint(deque(snapshot), seen_ids)

async def scrape_all_journals_rotating(context) -> None:
    """
    é¡ºåºæŠ“å–æ¯æœ¬æœŸåˆŠï¼Œè‡ªåŠ¨æ¢æµ‹å°¾é¡µï¼š
    - åˆ—è¡¨é¡µ/è¯¦æƒ…é¡µå‡ºç°ä¸€æ¬¡éªŒè¯ => éšæœºå†·å´ BACKOFF_BASE~BACKOFF_MAX ç§’ï¼Œç„¶ååŸåœ°é‡è¯•/ç»§ç»­
    - åˆ—è¡¨é¡µè¿ç»­ä¸¤é¡µæ— é“¾æ¥ï¼ˆä¸”éæŒ‘æˆ˜é¡µï¼‰ => åˆ¤å®šåˆ°å°¾é¡µ
    - ç½‘ç»œ/è¶…æ—¶é”™è¯¯ï¼šä¸åŠ é¡µï¼ŒåŸåœ°å†·å´åé‡è¯•
    - æ–­ç‚¹ï¼šç²¾ç¡®åˆ°â€œç¬¬ N é¡µçš„ç¬¬ K æ¡â€ï¼›Ctrl+C æ—¶ä¹Ÿä¼šè½ç›˜
    - ä¿å­˜è·¯å¾„ï¼šdata/<æœŸåˆŠå>/ï¼›æ–‡ä»¶åï¼š{abstract_id}_page{page}_NO.{ordinal}.txt
    """
    restored = load_checkpoint(CHECKPOINT_FILE)
    journals: List[Dict] = []

    if restored and restored.get("cursors"):
        print(f"ğŸ” å‘ç°æ–­ç‚¹æ–‡ä»¶ {CHECKPOINT_FILE}ï¼Œå°†ä»ä¸Šæ¬¡ä½ç½®ç»§ç»­ã€‚")
        for cur in restored["cursors"]:
            journals.append({
                "name": cur["name"],
                "jid": cur["jid"],
                "page": max(1, int(cur["page"])),
                "link_idx": max(0, int(cur["link_idx"])),
                "save_dir": os.path.join(DATA_DIR, cur["name"]),
            })
    else:
        for name, jid in JOURNAL_IDS.items():
            sp, _ = JOURNAL_PAGE_RANGE.get(name, (1, 1))
            journals.append({
                "name": name,
                "jid": jid,
                "page": sp,
                "link_idx": 0,
                "save_dir": os.path.join(DATA_DIR, name),
            })

    list_page = await context.new_page()
    await humanize_page(list_page)

    try:
        for i, cur in enumerate(journals):
            name = cur["name"]; jid = cur["jid"]; page_num = int(cur["page"])
            os.makedirs(cur["save_dir"], exist_ok=True)

            print(f"\n===== å¼€å§‹æœŸåˆŠ {name} (jid={jid})ï¼Œä»ç¬¬ {page_num} é¡µèµ· =====")

            empty_pages_in_a_row = 0

            while True:
                list_url = JOURNAL_URL_TEMPLATE.format(jid=jid, page=page_num)
                print(f"\nğŸŒ [{name}] ç¬¬ {page_num} é¡µ: {list_url}")

                # åˆ—è¡¨é¡µï¼šç½‘ç»œ/è¶…æ—¶é”™è¯¯ -> ä¸åŠ é¡µï¼ŒåŸåœ°é€€é¿é‡è¯•ï¼ˆéšæœºç§’æ•°ï¼‰
                try:
                    links = await load_links_on_page(list_page, list_url, LINK_SELECTORS)
                except Exception as e:
                    wait_s = random.uniform(BACKOFF_BASE, BACKOFF_MAX)
                    print(f"âš ï¸ åˆ—è¡¨åŠ è½½å¤±è´¥ï¼š{e}\n   â‡¢ å†·å´ {wait_s:.1f}s ååœ¨åŒä¸€é¡µé‡è¯• â€¦")
                    save_progress(journals, i, page_num, cur.get("link_idx", 0), seen_ids)
                    await asyncio.sleep(wait_s)
                    continue

                # æ— é“¾æ¥ï¼šæŒ‘æˆ˜ or çœŸç©ºé¡µ
                if not links:
                    try:
                        body = await list_page.inner_text("body")
                    except Exception:
                        body = ""

                    # éªŒè¯é¡µï¼šä¸åŠ é¡µï¼ŒåŸåœ°é€€é¿é‡è¯•ï¼ˆéšæœºç§’æ•°ï¼‰
                    if looks_like_challenge(body):
                        wait_s = random.uniform(BACKOFF_BASE, BACKOFF_MAX)
                        print(f"ğŸ§± åˆ—è¡¨é¡µç–‘ä¼¼éªŒè¯ï¼Œå†·å´ {wait_s:.1f}s åé‡è¯•å½“å‰é¡µ â€¦")
                        save_progress(journals, i, page_num, cur.get("link_idx", 0), seen_ids)
                        await asyncio.sleep(wait_s)
                        continue

                    # çœŸç©ºé¡µï¼šè¿ç»­ä¸¤æ¬¡æ‰åˆ¤å°¾é¡µ
                    empty_pages_in_a_row += 1
                    if empty_pages_in_a_row >= 2:
                        print(f"âœ… [{name}] è¿ç»­ä¸¤é¡µæ— æœ‰æ•ˆé“¾æ¥ï¼ˆåˆ°ç¬¬ {page_num} é¡µï¼‰ï¼Œåˆ¤å®šåˆ°å°¾é¡µï¼Œç»“æŸè¯¥åˆŠã€‚")
                        save_progress(journals, i, page_num, cur.get("link_idx", 0), seen_ids)
                        break
                    else:
                        print(f"ğŸ“­ [{name}] ç¬¬ {page_num} é¡µæ— æœ‰æ•ˆé“¾æ¥ï¼Œç¿»åˆ°ä¸‹ä¸€é¡µç¡®è®¤ã€‚")
                        save_progress(journals, i, page_num, 0, seen_ids)
                        cur["link_idx"] = 0
                        page_num += 1
                        continue
                else:
                    empty_pages_in_a_row = 0

                print(f"ğŸ“‘ [{name}] ç¬¬ {page_num} é¡µå…± {len(links)} æ¡æ–‡ç« é“¾æ¥")

                # è‹¥ä»æ–­ç‚¹æ¢å¤ï¼šè·³è¿‡å·²å¤„ç†å®Œçš„æœ¬é¡µå‰è‹¥å¹²æ¡ï¼ˆlink_idx è¡¨ç¤ºâ€œå·²å¤„ç†åˆ°çš„åºå·â€ï¼‰
                start_ordinal = max(1, int(cur.get("link_idx", 0)) + 1)

                for ordinal, link in enumerate(links, start=1):
                    if ordinal < start_ordinal:
                        continue  # è·³è¿‡å·²å¤„ç†æ¡ç›®

                    abs_id = extract_abstract_id_from_url(link) or f"unk_{ordinal}"
                    file_stem = make_filename(abs_id, page_num, ordinal)

                    # å»é‡ï¼ˆå¯é€‰ï¼‰
                    if ENABLE_GLOBAL_DEDUP and not abs_id.startswith("unk_"):
                        async with seen_ids_lock:
                            if abs_id in seen_ids:
                                print(f"â†©ï¸ å·²æŠ“è¿‡ abstract_id={abs_id}ï¼Œè·³è¿‡")
                                # è¿›åº¦å‰ç§»ï¼šå·²å¤„ç†åˆ° ordinal
                                cur["link_idx"] = ordinal
                                save_progress(journals, i, page_num, ordinal, seen_ids)
                                continue

                    # â€”â€” è¿™é‡Œå¼€å§‹ï¼šå‘½ä¸­éªŒè¯ â†’ å†·å´ â†’ åŸåœ°é‡è¯•åŒä¸€ç¯‡ â€”â€” #
                    attempts = 0
                    while True:
                        async with detail_sem:
                            saved, hit_challenge, _ = await fetch_article_text(
                                context=context,
                                url=link,
                                save_dir=cur["save_dir"],
                                file_stem=file_stem,
                            )

                        if saved:
                            # ä¿å­˜æˆåŠŸï¼šå†™å…¥å»é‡é›†åˆï¼ˆè‹¥å¯ç”¨ï¼‰
                            if ENABLE_GLOBAL_DEDUP and not abs_id.startswith("unk_"):
                                async with seen_ids_lock:
                                    seen_ids.add(abs_id)
                            # è¿›åº¦å‰ç§»å¹¶è½ç›˜
                            cur["link_idx"] = ordinal
                            save_progress(journals, i, page_num, ordinal, seen_ids)
                            break  # è·³å‡ºâ€œé‡è¯•åŒä¸€ç¯‡â€çš„ whileï¼Œè¿›å…¥ä¸‹ä¸€ä¸ª ordinal

                        # æœªä¿å­˜æˆåŠŸï¼šå¯èƒ½æ˜¯æŒ‘æˆ˜æˆ–è¶…æ—¶/å¼‚å¸¸ï¼ŒæŒ‰ç»Ÿä¸€ç­–ç•¥é‡è¯•
                        attempts += 1
                        if attempts >= RETRY_PER_ARTICLE:
                            print(f"â­ï¸ [{name}] ç¬¬ {page_num} é¡µ NO.{ordinal} é‡è¯• {attempts} æ¬¡ä»å¤±è´¥ï¼Œæ”¾å¼ƒè¯¥æ¡ã€‚")
                            # æ”¾å¼ƒè¯¥æ¡ï¼šå‰ç§»è¿›åº¦ï¼Œé¿å…æ­»å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€æ¡
                            cur["link_idx"] = ordinal
                            save_progress(journals, i, page_num, ordinal, seen_ids)
                            break

                        # ä»è¦é‡è¯•ï¼šéšæœºå†·å´åâ€œåŸåœ°é‡è¯•åŒä¸€ç¯‡â€ï¼ˆä¸å‰ç§» link_idxï¼‰
                        wait_s = random.uniform(BACKOFF_BASE, BACKOFF_MAX)
                        reason = "è¯¦æƒ…é¡µç–‘ä¼¼éªŒè¯" if hit_challenge else "è¯¦æƒ…é¡µå¤±è´¥/è¶…æ—¶"
                        print(f"ğŸ§± {reason}ï¼Œç¬¬ {attempts} æ¬¡é‡è¯•å‰å†·å´ {wait_s:.1f}s â€¦ï¼ˆä»å°†é‡è¯•åŒä¸€ç¯‡ï¼‰")
                        # æ–­ç‚¹è½ç›˜ï¼šä¿æŒå½“å‰ ordinalï¼ˆæœªå‰ç§»ï¼‰
                        save_progress(journals, i, page_num, cur.get("link_idx", 0), seen_ids)
                        await asyncio.sleep(wait_s)
                        # while True ç»§ç»­ï¼›ordinal ä¸å˜ â†’ â€œåŸåœ°é‡è¯•åŒä¸€ç¯‡â€

                # æœ¬é¡µå¤„ç†å®Œ -> ç¿»é¡µï¼ˆé¡µå†…ä½ç½®å½’é›¶ï¼‰
                cur["link_idx"] = 0
                save_progress(journals, i, page_num, 0, seen_ids)
                page_num += 1

            print(f"ğŸ¯ æœŸåˆŠ {name} å®Œæˆã€‚")

    except KeyboardInterrupt:
        try:
            if 'i' in locals() and 'page_num' in locals():
                lk = 0
                try:
                    lk = journals[i].get("link_idx", 0)
                except Exception:
                    pass
                save_progress(journals, i, page_num, lk, seen_ids)
            print("ğŸ›‘ æ•è·åˆ° Ctrl+Cï¼Œå·²ä¿å­˜æ–­ç‚¹ã€‚")
        except Exception as e:
            print(f"âš ï¸ Ctrl+C æ—¶ä¿å­˜æ–­ç‚¹å¤±è´¥ï¼š{e}")
    finally:
        try:
            await list_page.close()
        except Exception:
            pass

    print("\nğŸ‰ æ‰€æœ‰æœŸåˆŠå¤„ç†å®Œæˆ")
