from __future__ import annotations
import os, random, asyncio
from typing import List, Dict
from collections import deque
from .config import (
    DATA_DIR, JOURNAL_IDS, JOURNAL_PAGE_RANGE, JOURNAL_URL_TEMPLATE,
    BACKOFF_BASE, BACKOFF_MAX, RETRY_PER_PAGE, CHECKPOINT_FILE,
)
from .checkpoint import save_checkpoint, load_checkpoint
from .scraping import fetch_list_page_text

def _save_progress(journals: List[Dict], idx: int, page_num: int) -> None:
    cursors = []
    cur = journals[idx]
    cursors.append({
        "name": cur["name"], "jid": cur["jid"],
        "page": int(page_num), "end_page": 999999,
        "link_idx": 0, "save_dir": cur["save_dir"], "article_idx": 0,
    })
    for j in range(idx + 1, len(journals)):
        rest = journals[j]
        cursors.append({
            "name": rest["name"], "jid": rest["jid"],
            "page": int(rest.get("page", 1)), "end_page": 999999,
            "link_idx": 0, "save_dir": rest["save_dir"], "article_idx": 0,
        })
    save_checkpoint(deque(cursors), set())

async def scrape_journals_index_snapshot(context) -> None:
    """
    æŠŠâ€œç›®å½•é¡µçš„æ•´é¡µæ–‡æœ¬â€è½æˆ .txtï¼ˆä¸å…¨æ–‡ä¿å­˜é£æ ¼ä¸€è‡´ï¼‰ï¼Œé€é¡µç¿»é¡µã€‚
    å‘½ä¸­éªŒè¯ â†’ éšæœºå†·å´ â†’ åŸåœ°é‡è¯•åŒä¸€é¡µï¼›è¶…è¿‡é‡è¯•ä¸Šé™åˆ™è·³åˆ°ä¸‹ä¸€é¡µã€‚
    """
    restored = load_checkpoint(CHECKPOINT_FILE)
    journals: List[Dict] = []

    if restored and restored.get("cursors"):
        print(f"ğŸ” å‘ç°æ–­ç‚¹ {CHECKPOINT_FILE}ï¼Œä»ä¸Šæ¬¡ä½ç½®ç»§ç»­ã€‚")
        for cur in restored["cursors"]:
            journals.append({
                "name": cur["name"],
                "jid": cur["jid"],
                "page": max(1, int(cur["page"])),
                "save_dir": os.path.join(DATA_DIR, cur["name"]),
            })
    else:
        for name, jid in JOURNAL_IDS.items():
            sp, _ = JOURNAL_PAGE_RANGE.get(name, (1, 1))
            journals.append({
                "name": name, "jid": jid, "page": sp,
                "save_dir": os.path.join(DATA_DIR, name),
            })

    try:
        for i, cur in enumerate(journals):
            name, jid, page_num = cur["name"], cur["jid"], int(cur["page"])
            os.makedirs(cur["save_dir"], exist_ok=True)

            sp, ep = JOURNAL_PAGE_RANGE.get(name, (page_num, page_num))
            if page_num < sp: page_num = sp

            print(f"\n===== æœŸåˆŠ {name} (jid={jid})ï¼šä»ç¬¬ {page_num} é¡µå¼€å§‹ï¼Œä¿å­˜ç›®å½•é¡µæ•´é¡µæ–‡æœ¬ =====")

            while page_num <= ep:
                url = JOURNAL_URL_TEMPLATE.format(jid=jid, page=page_num)
                file_stem = f"list_{page_num:05d}"
                print(f"ğŸŒ [{name}] ç¬¬ {page_num} é¡µ: {url}")

                attempts = 0
                while True:
                    saved, hit_chal, sz = await fetch_list_page_text(
                        context=context,
                        url=url,
                        save_dir=cur["save_dir"],
                        file_stem=file_stem,
                    )

                    if saved:
                        print(f"ğŸ“ ä¿å­˜æˆåŠŸï¼š{name}/{file_stem}.html  ({sz} bytes)")
                        from .config import COOKIE_FILE
                        try:
                            await context.storage_state(path=COOKIE_FILE)
                        except Exception:
                            pass
                        _save_progress(journals, i, page_num + 1)   # âœ… æˆåŠŸæ‰å‰ç§»
                        page_num += 1
                        break

                    # æœªä¿å­˜åˆ°æ­£å¸¸ç›®å½•ï¼šæŒ‘æˆ˜æˆ–å¼‚å¸¸
                    attempts += 1
                    if attempts >= RETRY_PER_PAGE:
                        print(f"â­ï¸ æœ¬é¡µé‡è¯• {attempts} æ¬¡ä»å¤±è´¥ â†’ è·³è¿‡åˆ°ä¸‹ä¸€é¡µ")
                        _save_progress(journals, i, page_num + 1)   # æ”¾å¼ƒè¯¥é¡µï¼Œå‰ç§»
                        page_num += 1
                        break

                    # éšæœºå†·å´åâ€œåŸåœ°é‡è¯•åŒä¸€é¡µâ€
                    wait_s = random.uniform(BACKOFF_BASE, BACKOFF_MAX)
                    reason = "æŒ‘æˆ˜" if hit_chal else "å¼‚å¸¸/è¶…æ—¶"
                    print(f"ğŸ§± {reason} â†’ å†·å´ {wait_s:.1f}s ååŸåœ°é‡è¯•æœ¬é¡µ")
                    _save_progress(journals, i, page_num)          # ä¸å‰ç§»
                    await asyncio.sleep(wait_s)

            print(f"ğŸ¯ æœŸåˆŠ {name} å®Œæˆï¼ˆç›®å½•é¡µæ•´é¡µæ–‡æœ¬ä¿å­˜ï¼‰ã€‚")

    except KeyboardInterrupt:
        try:
            _save_progress(journals, i, page_num)
        except Exception:
            pass
        print("ğŸ›‘ æ•è·åˆ° Ctrl+Cï¼Œæ–­ç‚¹å·²ä¿å­˜ã€‚")

    print("\nğŸ‰ å…¨éƒ¨æœŸåˆŠç›®å½•é¡µæŠ“å–å®Œæˆï¼ˆæ•´é¡µæ–‡æœ¬ç‰ˆï¼‰")
