import csv
import re
import unicodedata
import argparse
import time
from pathlib import Path
from typing import Dict, Tuple, List, Optional
from collections import defaultdict
from functools import lru_cache

# ========= é…ç½® =========
ENGLISH_COUNTRIES = {"US", "GB", "AU", "CA"}
INSTITUTIONS_COLUMN = "affiliations"
INPUT_ENCODING = "utf-8-sig"
OUTPUT_ENCODING = "utf-8-sig"
FUZZY_THRESHOLD = 92
USE_FUZZY_DEFAULT = True

NO_AFFIL_PHRASE = "affiliation not provided to SSRN"
INDEPENDENT_KEYWORD = "Independent"

# åœç”¨è¯ï¼ˆç”¨äºæ¨¡ç³Šé˜¶æ®µè¿‡æ»¤ï¼Œé¿å…â€œuniversityâ€ç­‰è¿‡åº¦æ³›åŒ–é”®ï¼‰
STOP_TOKENS = {
    "the","of","and","university","college","school","institute","center","centre",
    "hospital","clinic","faculty","lab","laboratory","campus","graduate"
}

# ç»“æœç¼“å­˜ï¼šraw_inst -> Optional[(ror_id, country_code)]
_LOOKUP_MEMO: Dict[str, Optional[Tuple[str, str]]] = {}

# rapidfuzzï¼ˆå¯é€‰ï¼‰
try:
    from rapidfuzz import process, fuzz
    HAS_FUZZ = True
except Exception:
    HAS_FUZZ = False

# ========= è§„èŒƒåŒ–ï¼ˆæŸ¥è¯¢ä¾§ vs æ˜ å°„ä¾§åˆ†ç¦»ï¼‰ =========
def _basic_norm(s: str) -> str:
    if s is None:
        return ""
    s = unicodedata.normalize("NFKC", s).lower().strip()
    s = "".join(c for c in unicodedata.normalize("NFKD", s) if not unicodedata.combining(c))
    s = re.sub(r"[â€â€’â€“â€”âˆ’\-/:|,;]+", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s

_PARENS = re.compile(r"[ï¼ˆ(].*?[ï¼‰)]")
_THE = re.compile(r"^\bthe\b\s+")
# ä»…ç”¨äºâ€œæŸ¥è¯¢ä¾§â€çš„å°¾éƒ¨è£å‰ªï¼šé¿å…æŠŠâ€œUniversity Hospital Bonnâ€åç¼©æˆâ€œuniversityâ€
_DEPT_TAIL = re.compile(
    r"\b(department|school|faculty|graduate school|college|institute|center|centre|laboratory|lab|hospital|clinic|campus)\b.*$",
    flags=re.IGNORECASE,
)

@lru_cache(maxsize=100_000)
def query_key_canonical(s: str) -> str:
    """æŸ¥è¯¢ä¾§è§„èŒƒåŒ–ï¼šä¼šè£æ‰é™¢ç³»/åŒ»é™¢ç­‰å°¾å·´ï¼Œæå‡å¬å›ï¼Œä½†ä¸ä¼šå½±å“æ˜ å°„è¡¨é”®ã€‚"""
    s = _basic_norm(s)
    s = _PARENS.sub("", s)
    s = _THE.sub("", s)
    s = _DEPT_TAIL.sub("", s)
    s = s.strip()
    return re.sub(r"\s+", " ", s)

@lru_cache(maxsize=100_000)
def map_key_canonical(s: str) -> str:
    """æ˜ å°„è¡¨ä¾§è§„èŒƒåŒ–ï¼šä¸è£å°¾å·´ï¼Œä¿ç•™å…·ä½“æ€§ï¼Œé¿å…äº§ç”Ÿè¿‡åº¦æ³›åŒ–é”®ã€‚"""
    s = _basic_norm(s)
    s = _PARENS.sub("", s)
    s = _THE.sub("", s)
    s = s.strip()
    return re.sub(r"\s+", " ", s)

@lru_cache(maxsize=100_000)
def candidate_variants_both_keys(raw: str) -> List[str]:
    """
    ä¸ºâ€œç²¾ç¡®åŒ¹é…â€ç”ŸæˆæŸ¥è¯¢ä¾§ä¸æ˜ å°„ä¾§éƒ½å‹å¥½çš„é”®å€™é€‰ï¼š
    - å¯¹åŸæ–‡ã€å»æ‹¬å·ã€åˆ†éš”ç¬¦å‰ç¬¬ä¸€æ®µåˆ†åˆ«äº§å‡º
    - åŒæ—¶ç”Ÿæˆ query_key_canonical å’Œ map_key_canonical ä¸¤ç§è§„èŒƒåŒ–çš„å€™é€‰
    """
    raw = (raw or "").strip()
    cands = set()
    variants = []

    # åŸæ–‡
    variants.append(raw)

    # å»æ‹¬å·
    no_paren = _PARENS.sub("", raw)
    if no_paren != raw:
        variants.append(no_paren)

    # åˆ†éš”ç¬¦å‰ç¬¬ä¸€æ®µï¼ˆraw & no_parenï¼‰
    def first_cut(s: str) -> str:
        return re.split(r"[â€â€’â€“â€”\-:|,;/]", s, maxsplit=1)[0]

    variants.append(first_cut(raw))
    variants.append(first_cut(no_paren))

    # å»å‰å¯¼ theï¼ˆä¿é™©ï¼‰
    variants = [re.sub(r"^the\s+", "", v.strip(), flags=re.I) for v in variants if v.strip()]

    # å¯¹æ¯ä¸ª variant ç”Ÿæˆä¸¤ç§é”®ï¼šquery_key_canonical / map_key_canonical
    for v in variants:
        qk = query_key_canonical(v)
        mk = map_key_canonical(v)
        if qk:
            cands.add(qk)
        if mk:
            cands.add(mk)

    return [c for c in cands if c]

def _tokens(s: str) -> List[str]:
    return re.findall(r"[a-z0-9]+", s)

# ========= åŠ è½½æ˜ å°„ï¼ˆæ„å»ºå€’æ’ç´¢å¼•ï¼‰ =========
def load_mapping(tsv_path: Path) -> Dict[str, Tuple[str, str]]:
    """
    è¿”å›ï¼šname_key(æ˜ å°„ä¾§è§„èŒƒåŒ–) -> (ror_id, country_code)
    åŒæ—¶åœ¨å­—å…¸ä¸­åŠ å…¥ç‰¹æ®Šé”® "__TOKEN_INDEX__" å­˜æ”¾å€’æ’ç´¢å¼•ï¼Œç”¨äºç¼©å°æ¨¡ç³Šå€™é€‰é›†ã€‚
    """
    name2info: Dict[str, Tuple[str, str]] = {}
    with tsv_path.open("r", encoding="utf-8") as f:
        for line in f:
            parts = line.rstrip("\n").split("\t")
            if len(parts) != 3:
                continue
            name_key, rid, cc = parts
            key = map_key_canonical(name_key)  # é‡è¦ï¼šæ˜ å°„ä¾§è§„èŒƒåŒ–ï¼ˆä¸è£å°¾å·´ï¼‰
            if key and rid and (key not in name2info):
                name2info[key] = (rid, cc)

    # å€’æ’ç´¢å¼•ï¼štoken -> set(keys)
    token_index = defaultdict(set)
    for k in name2info.keys():
        for tok in _tokens(k):
            token_index[tok].add(k)

    # ç‰¹æ®Šé”®å­˜ç´¢å¼•
    name2info["__TOKEN_INDEX__"] = token_index  # type: ignore
    return name2info

# ========= æŸ¥æ‰¾ï¼šç²¾ç¡®ï¼ˆå¤šå˜ä½“ã€å¤šè§„èŒƒåŒ–ï¼‰ + æ¨¡ç³Šï¼ˆè£å‰ªå€™é€‰é›†ï¼‰ =========
def lookup_with_variants(name: str,
                         name2info: Dict[str, Tuple[str, str]],
                         use_fuzzy: bool = USE_FUZZY_DEFAULT,
                         fuzzy_threshold: int = FUZZY_THRESHOLD) -> Optional[Tuple[str, str]]:
    # ç»“æœç¼“å­˜ï¼ˆåŸºäºåŸå§‹è¾“å…¥å»ç©ºç™½ï¼‰
    memo_key = (name or "").strip()
    if memo_key in _LOOKUP_MEMO:
        return _LOOKUP_MEMO[memo_key]

    # ç²¾ç¡®ï¼šå˜ä½“ + åŒè§„èŒƒåŒ–é”®
    for cand in candidate_variants_both_keys(name or ""):
        if cand in name2info:
            _LOOKUP_MEMO[memo_key] = name2info[cand]
            return _LOOKUP_MEMO[memo_key]

    # æ¨¡ç³Šï¼ˆå¯é€‰ï¼‰ï¼šåªåœ¨â€œæœ‰è¯äº¤é›†â€çš„å€™é€‰é”®é‡Œæ‰¾ï¼Œå¹¶è®¾ç½® score_cutoff æ—©åœ
    if use_fuzzy and HAS_FUZZ and name2info:
        q = query_key_canonical(name or "")
        if 4 <= len(q) <= 128:
            token_index = name2info.get("__TOKEN_INDEX__")  # type: ignore
            if token_index:
                qtoks_all = set(_tokens(q))
                qtoks = {t for t in qtoks_all if t not in STOP_TOKENS}

                # æ”¶é›†å€™é€‰ï¼šæŒ‰ï¼ˆæ‰€æœ‰è¯ï¼‰æ‹¿å€™é€‰ï¼Œå†ç”¨â€œå»åœç”¨è¯åçš„äº¤é›†>=1â€è¿‡æ»¤
                cands = set()
                for t in qtoks_all:
                    vals = token_index.get(t)  # å¯èƒ½æ˜¯ set / list / None
                    if vals:
                        cands.update(vals)

                filtered = []
                for k in cands:
                    ktoks_all = set(_tokens(k))
                    ktoks = {t for t in ktoks_all if t not in STOP_TOKENS}
                    # è¿‡æ»¤æ‰æçŸ­å€™é€‰ï¼ˆé¿å…â€œuniversityâ€è¿™ç±»é”®ï¼‰
                    if len(ktoks_all) <= 1:
                        continue
                    # è¦æ±‚è‡³å°‘1ä¸ªéåœç”¨è¯å…±è¯
                    if len(qtoks & ktoks) >= 1:
                        filtered.append(k)

                # â˜… é»˜è®¤å€™é€‰æ—¶ï¼Œæ’é™¤ç‰¹æ®Šé”®ï¼›å¹¶åœ¨æœ«å°¾ç»Ÿä¸€æ£€æŸ¥æ˜¯å¦ä¸ºç©º
                all_keys = [k for k in name2info.keys() if not (isinstance(k, str) and k.startswith("__"))]
                search_space = filtered if filtered else (list(cands) if cands else all_keys)
            else:
                # â˜… æ— å€’æ’ç´¢å¼•æ—¶ä¹Ÿæ’é™¤ç‰¹æ®Šé”®
                search_space = [k for k in name2info.keys() if not (isinstance(k, str) and k.startswith("__"))]

            # â˜… å€™é€‰å¯èƒ½ä¸ºç©ºï¼šç›´æ¥è¿”å› None
            if not search_space:
                _LOOKUP_MEMO[memo_key] = None
                return None

            # â˜… å®‰å…¨è§£åŒ…ï¼šextractOne å¯èƒ½è¿”å› None
            res = process.extractOne(
                q,
                search_space,
                scorer=fuzz.token_set_ratio,  # å¦‚éœ€æ›´ä¿å®ˆå¯æ¢ token_sort_ratio
                score_cutoff=fuzzy_threshold,
                processor=None  # æˆ‘ä»¬å·²ç»åšäº†è§„èŒƒåŒ–
            )
            if res is None:
                _LOOKUP_MEMO[memo_key] = None
                return None

            key = res[0]  # å…¼å®¹ v2/v3 çš„å†™æ³•
            _LOOKUP_MEMO[memo_key] = name2info[key]
            return _LOOKUP_MEMO[memo_key]

    _LOOKUP_MEMO[memo_key] = None
    return None


# ========= å•å…ƒæ ¼åˆ¤å®š =========
def judge_institutions_cell(institutions_cell: str,
                            name2info: Dict[str, Tuple[str, str]],
                            english_countries=ENGLISH_COUNTRIES,
                            use_fuzzy: bool = USE_FUZZY_DEFAULT,
                            fuzzy_threshold: int = FUZZY_THRESHOLD):
    raw_full = (institutions_cell or "").strip()
    if not raw_full:
        return ("unknown", [], [], [], "none", 0)

    # æ•´æ ¼ç­‰äº â€œaffiliation not provided to SSRNâ€ â†’ unknownï¼ˆä¿æŒä¸å˜ï¼‰
    if _basic_norm(raw_full) == _basic_norm(NO_AFFIL_PHRASE):
        return ("unknown", [], [], [], "none", 0)

    # æ‹†åˆ†å¤šæœºæ„ï¼ˆåˆ†å·ï¼‰
    inst_raw_list = [x.strip() for x in raw_full.split(";") if x.strip()]

    matched_countries: List[str] = []
    matched_ids: List[str] = []
    unmatched: List[str] = []

    for inst in inst_raw_list:
        # --- NEW: å¯¹äºç‹¬ç«‹ç ”ç©¶è€…ï¼Œä½œä¸ºä¸€ä¸ªâ€œå‘½ä¸­é¡¹â€å†™å…¥å›½å®¶åºåˆ— ---
        if inst.strip().lower() == INDEPENDENT_KEYWORD.lower() or \
           inst.strip().lower() == "independent researcher":
            matched_countries.append("Independent")
            # ä¸è¿½åŠ  ROR IDï¼›ä¹Ÿä¸è®¡å…¥ unmatched
            continue

        # å…¶ä»–æœºæ„ä»æ—§èµ°æ˜ å°„/æ¨¡ç³ŠåŒ¹é…
        info = lookup_with_variants(inst, name2info, use_fuzzy=use_fuzzy, fuzzy_threshold=fuzzy_threshold)
        if info:
            rid, cc = info
            matched_ids.append(rid)
            matched_countries.append(cc or "")
        else:
            unmatched.append(inst)

    # â€”â€” æ•´ä½“æ ‡ç­¾åˆ¤å®šï¼ˆä¿æŒä¸å˜ï¼›Independent ä¸å½±å“ strong/weak/unknownï¼‰â€”â€”
    # æ³¨æ„ï¼šå¦‚æœæ•´è¡Œåªæœ‰ "Independent"/"Independent researcher" è€Œæ— å…¶ä»–å‘½ä¸­ï¼Œ
    # åˆ™ matched_ids ä¸ºç©ºã€æ²¡æœ‰è‹±è¯­å›½å®¶ â†’ æŒ‰åŸé€»è¾‘ä¸º "unknown"
    if matched_countries and all((c or "").strip().lower() == "independent" for c in matched_countries):
        label = "independent"
    else:
        if not inst_raw_list:
            label = "unknown"
        elif any(cc in english_countries for cc in matched_countries):
            label = "strong"
        elif len(matched_ids) == 0:
            label = "unknown"
        else:
            label = "weak"

    # match_status ä»…æŒ‰â€œæ˜ å°„å‘½ä¸­æ•°â€è¯„ä¼°ï¼ˆIndependent ä¸è®¡å…¥ï¼‰
    mcount = len(matched_ids)
    if mcount == 0:
        mstatus = "none"
    elif mcount == len([x for x in inst_raw_list if x.strip().lower() not in {"independent", "independent researcher"}]):
        mstatus = "full"
    else:
        mstatus = "partial"

    return (label, matched_countries, matched_ids, unmatched, mstatus, mcount)


# ========= è¿›åº¦å·¥å…· =========
def precount_rows(csv_path: Path, encoding: str, has_header: bool = True) -> int:
    cnt = 0
    with csv_path.open("r", encoding=encoding, newline="") as f:
        for _ in f:
            cnt += 1
    return max(0, cnt - (1 if has_header else 0))

# ========= ä¸»ç¨‹åº =========
def main():
    ap = argparse.ArgumentParser(description="Judge English background with caching, pruned fuzzy, and split canonicalization for mapping/query.")
    ap.add_argument("-i", "--input", required=True, help="Path to papers CSV (must contain affiliations column unless --institutions-col is set).")
    ap.add_argument("-m", "--mapping", default="ror_name_country.tsv", help="Normalized mapping TSV (name_key\\tror_id\\tcountry_code).")
    ap.add_argument("-o", "--output", default=None, help="Output CSV path (default: <input>_with_English_bg.csv).")
    ap.add_argument("--encoding-in", default=INPUT_ENCODING, help=f"Input CSV encoding (default: {INPUT_ENCODING}).")
    ap.add_argument("--encoding-out", default=OUTPUT_ENCODING, help=f"Output CSV encoding (default: {OUTPUT_ENCODING}).")
    ap.add_argument("--institutions-col", default=INSTITUTIONS_COLUMN, help=f"Institutions column name (default: {INSTITUTIONS_COLUMN}).")
    ap.add_argument("--no-fuzzy", action="store_true", help="Disable fuzzy fallback even if rapidfuzz is installed.")
    ap.add_argument("--fuzzy-threshold", type=int, default=FUZZY_THRESHOLD, help=f"Fuzzy score threshold (default: {FUZZY_THRESHOLD}).")
    ap.add_argument("--progress-every", type=int, default=1000, help="Print speed/progress every N rows (default: 1000).")
    ap.add_argument("--precount", action="store_true", help="Pre-count total rows to show ETA (slightly slower startup).")
    args = ap.parse_args()

    input_path = Path(args.input)
    mapping_path = Path(args.mapping)
    output_path = Path(args.output) if args.output else input_path.with_name(input_path.stem + "_with_English_bg.csv")

    if not input_path.exists():
        raise SystemExit(f"âŒ Input CSV not found: {input_path}")
    if not mapping_path.exists():
        raise SystemExit(f"âŒ Mapping TSV not found: {mapping_path}")

    print(f"ğŸ“¥ Loading mapping: {mapping_path}")
    name2info = load_mapping(mapping_path)
    print(f"âœ… Mapping loaded: {len([k for k in name2info.keys() if not k.startswith('__')]):,} keys")

    use_fuzzy = (not args.no_fuzzy) and HAS_FUZZ
    if (not HAS_FUZZ) and (not args.no_fuzzy):
        print("â„¹ï¸ rapidfuzz not installed; running without fuzzy fallback.\n    Install with: pip install rapidfuzz")

    total_rows = None
    if args.precount:
        print("ğŸ” Pre-counting total rows for ETAâ€¦")
        total_rows = precount_rows(input_path, args.encoding_in)
        print(f"ğŸ§® Total data rows (excluding header): {total_rows:,}")

    with input_path.open("r", encoding=args.encoding_in, newline="") as fin, \
         output_path.open("w", encoding=args.encoding_out, newline="") as fout:
        reader = csv.DictReader(fin)
        fieldnames = list(reader.fieldnames or [])
        if args.institutions_col not in fieldnames:
            raise SystemExit(f"âŒ Column not found in input CSV: {args.institutions_col}")

        # é™„åŠ è¾“å‡ºåˆ—
        for col in ("english_background", "matched_countries", "matched_ror_ids",
                    "unmatched_institutions", "match_status", "match_count"):
            if col not in fieldnames:
                fieldnames.append(col)

        writer = csv.DictWriter(fout, fieldnames=fieldnames)
        writer.writeheader()

        rows = 0
        t0 = time.time()
        last_t = t0
        last_rows = 0

        for row in reader:
            rows += 1
            label, countries, rids, unmatched, mstatus, mcount = judge_institutions_cell(
                row.get(args.institutions_col, ""),
                name2info,
                english_countries=ENGLISH_COUNTRIES,
                use_fuzzy=use_fuzzy,
                fuzzy_threshold=args.fuzzy_threshold
            )
            row["english_background"] = label
            row["matched_countries"] = ",".join([c for c in countries if c])
            row["matched_ror_ids"] = ",".join(rids)
            row["unmatched_institutions"] = ";".join(unmatched)
            row["match_status"] = mstatus
            row["match_count"] = str(mcount)
            writer.writerow(row)

            # è¿›åº¦ä¸é€Ÿåº¦
            if args.progress_every > 0 and (rows % args.progress_every == 0):
                now = time.time()
                elapsed = now - t0
                batch_elapsed = now - last_t
                avg_speed = rows / elapsed if elapsed > 0 else 0.0
                batch_rows = rows - last_rows
                batch_speed = batch_rows / batch_elapsed if batch_elapsed > 0 else 0.0
                eta_str = ""
                if total_rows is not None and avg_speed > 0:
                    remaining = max(0, total_rows - rows)
                    eta_sec = remaining / avg_speed
                    eta_str = f" | ETA ~ {eta_sec/60:.1f} min"
                print(f"â±ï¸ {rows:,} rows | avg {avg_speed:.2f} rows/s | last {batch_speed:.2f} rows/s{eta_str}")
                last_t = now
                last_rows = rows

    t1 = time.time()
    elapsed = t1 - t0
    avg_speed = (rows / elapsed) if elapsed > 0 else 0.0
    print(f"âœ… Done. Processed {rows:,} rows in {elapsed:.2f}s (avg {avg_speed:.2f} rows/s).")
    print(f"ğŸ“„ Output: {output_path}")

if __name__ == "__main__":
    import sys
    sys.argv = [
        sys.argv[0],
        "-i", "E:/SSRNPaperResearch/data/isn/result/TechnologySystemseJournal.csv",
        "-m", "E:/SSRNPaperResearch/data/ror_name_country_norm.tsv",
        "--progress-every", "500",
        "--precount"
    ]
    main()