from __future__ import annotations
import sys
import re
from pathlib import Path
from typing import List, Optional, Set
from bs4 import BeautifulSoup
import csv
import unicodedata

# å¯é€‰ç¼–ç æ¢æµ‹
try:
    from charset_normalizer import from_bytes as detect_from_bytes
except Exception:
    detect_from_bytes = None

DELIM = ';'  # å¤šä½œè€…åˆ†éš”
WILEY_BASE = "https://onlinelibrary.wiley.com"
WILEY_EXCLUDE_TITLES = {"Issue Information", "IN THIS ISSUE"}  # ä¸¥æ ¼ç­‰å€¼è¿‡æ»¤

def normalize_space(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()

def nfc(text: str) -> str:
    return unicodedata.normalize('NFC', text)

def read_html_text(path: Path) -> str:
    """ç¨³å¥è¯»å–æœ¬åœ° HTML"""
    raw = path.read_bytes()
    try:
        txt = raw.decode('utf-8')
    except UnicodeDecodeError:
        if detect_from_bytes is not None:
            best = detect_from_bytes(raw).best()
            txt = str(best) if best is not None else raw.decode('latin-1', errors='replace')
        else:
            txt = raw.decode('latin-1', errors='replace')
    return nfc(txt)

def parse_wiley_list_html(soup: BeautifulSoup, source_file: str) -> List[dict]:
    """
    è§£æ Wiley TOC é¡µé¢çš„æ¯æ¡æ–‡ç« å¡ç‰‡ï¼Œè¿”å›ç»“æ„åŒ–è®°å½•
    é€‰æ‹©å™¨è¯´æ˜ï¼ˆä»¥å½“å‰ Wiley TOC DOM ä¸ºå‡†ï¼‰ï¼š
      - å¡ç‰‡ï¼šdiv.issue-item
      - æ ‡é¢˜ä¸é“¾æ¥ï¼ša.issue-item__titleï¼ˆhref é€šå¸¸ä»¥ /doi/ å¼€å¤´ï¼‰
      - ä½œè€…ï¼š.loa .author-styleï¼ˆå¤šä½œè€…ï¼‰
      - é¡µç ï¼šli.page-range span:nth-of-type(2)
      - å‡ºç‰ˆæ—¥æœŸï¼šli.ePubDate span:nth-of-type(2)ï¼ˆå³ First Publishedï¼‰
    """
    out: List[dict] = []

    for item in soup.select("div.issue-item"):
        a = item.select_one("a.issue-item__title")
        if not a:
            continue

        title = normalize_space(a.get_text())
        # è¿‡æ»¤åˆŠè®¯/ç›®å½•ç±»å ä½é¡¹
        if title in WILEY_EXCLUDE_TITLES:
            continue

        href = a.get("href", "")
        # ç›¸å¯¹é“¾æ¥è¡¥å…¨æˆç»å¯¹ URL
        if href.startswith("/"):
            url = WILEY_BASE + href
        else:
            url = href or ""

        # ä½œè€…
        authors = [normalize_space(x.get_text()) for x in item.select(".loa .author-style")]
        authors_str = DELIM.join([nfc(x) for x in authors]) if authors else ""

        # é¡µç ï¼ˆå¦‚ 449-470ï¼‰
        pages_span = item.select_one("li.page-range span:nth-of-type(2)")
        pages = normalize_space(pages_span.get_text()) if pages_span else ""

        # å‡ºç‰ˆæ—¥æœŸï¼ˆFirst Published: 27 July 2025ï¼‰
        pub_span = item.select_one("li.ePubDate span:nth-of-type(2)")
        published = normalize_space(pub_span.get_text()) if pub_span else ""

        out.append({
            "id": url,          # å”¯ä¸€æ ‡è¯†ï¼šæ–‡ç«  URL
            "title": title,
            "authors": authors_str,
            "pages": pages,
            "published": published,
            "source_file": source_file,
        })

    return out

def parse_one_list_html(path: Path) -> List[dict]:
    html = read_html_text(path)
    try:
        soup = BeautifulSoup(html, 'lxml')
    except Exception:
        soup = BeautifulSoup(html, 'html.parser')
    return parse_wiley_list_html(soup, path.name)

def main(input_dir: str) -> Path:
    root = Path(input_dir).resolve()
    if not root.is_dir():
        raise SystemExit(f"Not a directory: {root}")

    result_dir = root.parent / 'result'
    result_dir.mkdir(parents=True, exist_ok=True)
    out_csv = result_dir / f"{root.name}.csv"

    files: List[Path] = sorted(root.rglob('*.html'))
    total_files = len(files)
    if total_files == 0:
        print("âš ï¸ æœªåœ¨è¯¥ç›®å½•ä¸‹æ‰¾åˆ°ä»»ä½• .html æ–‡ä»¶ã€‚")
        return out_csv

    print(f"ğŸ” å…±å‘ç° {total_files} ä¸ª HTML æ–‡ä»¶ï¼Œå°†å¼€å§‹è§£æâ€¦â€¦")
    last_dir: Optional[Path] = None
    rows: List[dict] = []

    for idx, p in enumerate(files, start=1):
        if p.parent != last_dir:
            last_dir = p.parent
            if last_dir != root:
                print(f"ğŸ“‚ æ­£åœ¨æ‰«æå­ç›®å½•ï¼š{last_dir}")
        rel = p.relative_to(root)
        print(f"[{idx}/{total_files}] è§£æï¼š{rel}")
        try:
            rows.extend(parse_one_list_html(p))
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥ï¼ˆè·³è¿‡ï¼‰{rel}: {e}")

    # ===== å»é‡ï¼ˆæŒ‰ idï¼Œå³ URLï¼‰=====
    total_rows = len(rows)
    seen: Set[str] = set()
    dedup_rows: List[dict] = []
    for r in rows:
        rid = (r.get('id') or '').strip()
        if rid and rid in seen:
            continue
        if rid:
            seen.add(rid)
        dedup_rows.append(r)

    # ===== å†™å…¥ CSV =====
    fieldnames = ['id', 'title', 'authors', 'pages', 'published', 'source_file']
    with out_csv.open('w', newline='', encoding='utf-8-sig') as f:
        w = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
        w.writeheader()
        for r in dedup_rows:
            cleaned = {}
            for k, v in r.items():
                if isinstance(v, str):
                    cleaned[k] = nfc(normalize_space(v))
                else:
                    cleaned[k] = v
            w.writerow(cleaned)

    # ===== æ±‡æ€» =====
    print("\n===== ç»Ÿè®¡æ±‡æ€» =====")
    print(f"ğŸ“„ åŸå§‹è§£æè®°å½•æ€»æ•°ï¼š{total_rows}")
    print(f"ğŸ§¹ å»é‡åè¾“å‡ºè®°å½•æ•°ï¼š{len(dedup_rows)}")
    if total_rows > 0:
        dup_num = total_rows - len(dedup_rows)
        rate = dup_num / total_rows * 100
        print(f"ğŸ” é‡å¤æ¡æ•°ï¼š{dup_num}ï¼ˆçº¦ {rate:.2f}%ï¼‰")

    return out_csv

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print('Usage: python parse_wiley_dir.py <folder_with_html_files>')
        sys.exit(1)
    output = main(sys.argv[1])
    print(f"\nâœ… Done. CSV saved to: {output}")

