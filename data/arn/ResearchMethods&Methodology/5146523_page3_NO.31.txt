Skip to main content
Product & Services
Research Paper Series
Site Subscriptions
Sponsored Services
Jobs & Announcements
Conference Papers
Partners in Publishing
First Look
Subscribe
Submit a paper
Browse
Rankings
Top Papers
Top Authors
Top Organizations
Blog↗
Contact
Create account
Sign in
Download This Paper
 
Open PDF in Browser
 Add Paper to My Library
Share:    
How to Design and Employ Specialized Large Language Models for Accounting and Tax Research: The Example of TaxBERT

69 Pages Posted: 22 Apr 2025

Frank Hechtner

Friedrich-Alexander-Universität Erlangen-Nürnberg

Lukas Schmidt

Constructor University Bremen

Andreas Seebeck

Jacobs University Bremen

Marius Weiß

Friedrich-Alexander-Universität Erlangen-Nürnberg

Date Written: February 20, 2025

Abstract

Large Language Models (LLMs) are emerging as the new gold standard for accounting researchers utilizing Natural Language Processing (NLP) techniques to analyze the characteristics of financial and non-financial disclosures. This study provides an 'A-to-Z' description of how to design and employ specialized Bidirectional Encoder Representation of Transformers (BERT) models that are environmentally sustainable and practically feasible for accounting and tax researchers. We begin by highlighting some NLP-related key challenges, pitfalls, and shortcomings. Next, we provide a user's guide to LLMs and BERT models. Using the case-based approach of TaxBERT, a domain-specific LLM pretrained for analyzing qualitative corporate tax disclosures, we provide a detailed discussion on the efficient design, evaluation, and application of specialized BERT models tailored to specific domains. We show that TaxBERT leads to significant performance improvements over generic LLMs, FinBERT, and traditional bag of words approaches. By showcasing the development as well as the potential of domain-specific models, we aim to inspire researchers to develop and apply specialized LLMs in their research.

Online appendix:

GitHub guide: https://github.com/TaxBERT/TaxBERT

Hugging Face model: https://huggingface.co/mariusweiss/TaxBERT

Keywords: large language models, machine learning, tax disclosure, text mining, natural language processing

Suggested Citation:

Hechtner, Frank and Schmidt, Lukas and Seebeck, Andreas and Weiß, Marius, How to Design and Employ Specialized Large Language Models for Accounting and Tax Research: The Example of TaxBERT (February 20, 2025). Available at SSRN: https://ssrn.com/abstract=5146523 or http://dx.doi.org/10.2139/ssrn.5146523
Download This Paper
 
Open PDF in Browser
85 References
A A Acito , M Nessa
Law Firms as Tax Planning Service Providers
The Accounting Review , volume 97 , issue 4 , p. 1 - 26 Posted: 2022
E Alsentzer , J R Murphy , W Boag , W.-H Weng , D Jin , T Naumann , M B Mcdermott
Publicly Available Clinical BERT Embeddings Posted: 2019
D Araci
FinBERT: Financial Sentiment Analysis with Pre-trained Language Models Posted: 2019
M Azimi , A Agrawal
Is Positive Sentiment in Corporate Annual Reports Informative? Evidence from Deep Learning
The Review of Asset Pricing Studies , volume 11 , issue 4 , p. 762 - 805 Posted: 2021
Load more
1 Citations
Eva Blondeel , Patricia Everaert , Evelien Opdecam
A practical guide to implementing ChatGPT as a secondary coder in qualitative research
International Journal of Accounting Information Systems , volume 56 , p. 100754 Posted: 2025
Crossref
Load more
Do you have a job opening that you would like to promote on SSRN?
Place Job Opening
Paper statistics
DOWNLOADS
559
ABSTRACT VIEWS
2,124
RANK
112,997
1 Citations
85 References
PlumX Metrics
Related eJournals

Financial Accounting eJournal

Follow

S&P Global Market Intelligence Research Paper Series

Follow
 
Feedback 
Submit a Paper 
Section 508 Text Only Pages
SSRN Quick Links
SSRN Solutions
Research Paper Series
Conference Papers
Partners in Publishing
Jobs & Announcements
Special Topic Hubs
SSRN Rankings
Top Papers
Top Authors
Top Organizations
About SSRN
Network Directors
Announcements
Contact us
FAQs
  
Copyright Terms and Conditions Privacy Policy

All content on this site: Copyright © 2024 Elsevier Inc., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the relevant licensing terms apply.

We use cookies to help provide and enhance our service and tailor content.

To learn more, visit Cookie settings | Your Privacy Choices. 

We use cookies that are necessary to make our site work. We may also use additional cookies to analyze, improve, and personalize our content and your digital experience. For more information, see ourCookie Policy
Cookie Settings
Accept all cookies